{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e076bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8e3ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>angle</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>facepart</th>\n",
       "      <th>bbox_x_min</th>\n",
       "      <th>bbox_y_min</th>\n",
       "      <th>bbox_x_max</th>\n",
       "      <th>bbox_y_max</th>\n",
       "      <th>forehead_wrinkle</th>\n",
       "      <th>partname</th>\n",
       "      <th>image_name</th>\n",
       "      <th>label_name</th>\n",
       "      <th>adj_forehead_wrinkle</th>\n",
       "      <th>adj_forehead_wrinkle_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>F</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>469.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>3</td>\n",
       "      <td>이마</td>\n",
       "      <td>0001_01_F.jpg</td>\n",
       "      <td>0001_01_F_01.json</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>L15</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>369.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>3</td>\n",
       "      <td>이마</td>\n",
       "      <td>0001_01_L15.jpg</td>\n",
       "      <td>0001_01_L15_01.json</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>L30</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>404.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>1159.0</td>\n",
       "      <td>3</td>\n",
       "      <td>이마</td>\n",
       "      <td>0001_01_L30.jpg</td>\n",
       "      <td>0001_01_L30_01.json</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>R15</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>494.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>1772.0</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>3</td>\n",
       "      <td>이마</td>\n",
       "      <td>0001_01_R15.jpg</td>\n",
       "      <td>0001_01_R15_01.json</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>R30</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>744.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>1755.0</td>\n",
       "      <td>1175.0</td>\n",
       "      <td>3</td>\n",
       "      <td>이마</td>\n",
       "      <td>0001_01_R30.jpg</td>\n",
       "      <td>0001_01_R30_01.json</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43381</th>\n",
       "      <td>1100</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>504.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>1</td>\n",
       "      <td>이마</td>\n",
       "      <td>1100_01_F.jpg</td>\n",
       "      <td>1100_01_F_01.json</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43390</th>\n",
       "      <td>1100</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>L15</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>339.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>1</td>\n",
       "      <td>이마</td>\n",
       "      <td>1100_01_L15.jpg</td>\n",
       "      <td>1100_01_L15_01.json</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43399</th>\n",
       "      <td>1100</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>L30</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>327.0</td>\n",
       "      <td>843.0</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>1</td>\n",
       "      <td>이마</td>\n",
       "      <td>1100_01_L30.jpg</td>\n",
       "      <td>1100_01_L30_01.json</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>1100</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>R15</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>671.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>1</td>\n",
       "      <td>이마</td>\n",
       "      <td>1100_01_R15.jpg</td>\n",
       "      <td>1100_01_R15_01.json</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43417</th>\n",
       "      <td>1100</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>R30</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>1</td>\n",
       "      <td>869.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>1194.0</td>\n",
       "      <td>1</td>\n",
       "      <td>이마</td>\n",
       "      <td>1100_01_R30.jpg</td>\n",
       "      <td>1100_01_R30_01.json</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4825 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id gender  age angle  width  height  facepart  bbox_x_min  \\\n",
       "1         1      F   55     F   2136    3216         1       469.0   \n",
       "10        1      F   55   L15   2136    3216         1       369.0   \n",
       "19        1      F   55   L30   2136    3216         1       404.0   \n",
       "28        1      F   55   R15   2136    3216         1       494.0   \n",
       "37        1      F   55   R30   2136    3216         1       744.0   \n",
       "...     ...    ...  ...   ...    ...     ...       ...         ...   \n",
       "43381  1100      F   28     F   2136    3216         1       504.0   \n",
       "43390  1100      F   28   L15   2136    3216         1       339.0   \n",
       "43399  1100      F   28   L30   2136    3216         1       327.0   \n",
       "43408  1100      F   28   R15   2136    3216         1       671.0   \n",
       "43417  1100      F   28   R30   2136    3216         1       869.0   \n",
       "\n",
       "       bbox_y_min  bbox_x_max  bbox_y_max  forehead_wrinkle partname  \\\n",
       "1           661.0      1638.0      1197.0                 3       이마   \n",
       "10          595.0      1540.0      1176.0                 3       이마   \n",
       "19          561.0      1427.0      1159.0                 3       이마   \n",
       "28          573.0      1772.0      1131.0                 3       이마   \n",
       "37          577.0      1755.0      1175.0                 3       이마   \n",
       "...           ...         ...         ...               ...      ...   \n",
       "43381       879.0      1629.0      1248.0                 1       이마   \n",
       "43390       847.0      1400.0      1201.0                 1       이마   \n",
       "43399       843.0      1261.0      1193.0                 1       이마   \n",
       "43408       833.0      1764.0      1209.0                 1       이마   \n",
       "43417       797.0      1805.0      1194.0                 1       이마   \n",
       "\n",
       "            image_name           label_name  adj_forehead_wrinkle  \\\n",
       "1        0001_01_F.jpg    0001_01_F_01.json                   1.0   \n",
       "10     0001_01_L15.jpg  0001_01_L15_01.json                   1.0   \n",
       "19     0001_01_L30.jpg  0001_01_L30_01.json                   1.0   \n",
       "28     0001_01_R15.jpg  0001_01_R15_01.json                   1.0   \n",
       "37     0001_01_R30.jpg  0001_01_R30_01.json                   1.0   \n",
       "...                ...                  ...                   ...   \n",
       "43381    1100_01_F.jpg    1100_01_F_01.json                   0.0   \n",
       "43390  1100_01_L15.jpg  1100_01_L15_01.json                   0.0   \n",
       "43399  1100_01_L30.jpg  1100_01_L30_01.json                   0.0   \n",
       "43408  1100_01_R15.jpg  1100_01_R15_01.json                   0.0   \n",
       "43417  1100_01_R30.jpg  1100_01_R30_01.json                   0.0   \n",
       "\n",
       "       adj_forehead_wrinkle_2  \n",
       "1                           1  \n",
       "10                          1  \n",
       "19                          1  \n",
       "28                          1  \n",
       "37                          1  \n",
       "...                       ...  \n",
       "43381                       0  \n",
       "43390                       0  \n",
       "43399                       0  \n",
       "43408                       0  \n",
       "43417                       0  \n",
       "\n",
       "[4825 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_forehead = pd.read_csv(\"new_df_forehead.csv\", index_col = 0)\n",
    "\n",
    "# NaN 값을 0이나 다른 값으로 채운 후 변환\n",
    "df_forehead['forehead_wrinkle'] = df_forehead['forehead_wrinkle'].fillna(0).astype(int)\n",
    "\n",
    "# NaN을 유지하면서 변환 (nullable Int64 type)\n",
    "df_forehead['forehead_wrinkle'] = df_forehead['forehead_wrinkle'].astype('Int64')\n",
    "\n",
    "# NaN 값을 0이나 다른 값으로 채운 후 변환\n",
    "df_forehead['adj_forehead_wrinkle_2'] = df_forehead['adj_forehead_wrinkle_2'].fillna(0).astype(int)\n",
    "\n",
    "# NaN을 유지하면서 변환 (nullable Int64 type)\n",
    "df_forehead['adj_forehead_wrinkle_2'] = df_forehead['adj_forehead_wrinkle_2'].astype('Int64')\n",
    "\n",
    "\n",
    "# df_forehead['image_name'] = df_forehead['image_name'].apply(lambda x: \"_\".join(x.split('_')[0:3]) + '.jpg')\n",
    "\n",
    "df_forehead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea74840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IntegerArray>\n",
      "[3, 1, 0, 2, 4, 5, 6]\n",
      "Length: 7, dtype: Int64\n",
      "<IntegerArray>\n",
      "[1, 0, 2, 3]\n",
      "Length: 4, dtype: Int64\n",
      "class weight for each label : {1: 2.46, 0: 2.46, 2: 12.7, 3: 9.28}\n"
     ]
    }
   ],
   "source": [
    "# original label\n",
    "print(df_forehead['forehead_wrinkle'].unique())\n",
    "\n",
    "#label rebalancing\n",
    "print(df_forehead['adj_forehead_wrinkle_2'].unique()) ### 그래프로 하면 더 좋았겠음\n",
    "\n",
    "class_counts = df_forehead['adj_forehead_wrinkle_2'].value_counts(sort=False).to_dict()\n",
    "num_samples = sum(class_counts.values())\n",
    "class_weights = {l: round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "\n",
    "#print(f'cls_cnts: {len(class_counts)}\\n num_samples:{num_samples}')\n",
    "\n",
    "labels =  df_forehead['adj_forehead_wrinkle_2'].to_list()\n",
    "weights = [class_weights[labels[i]] for i in range(int(num_samples))] \n",
    "sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "\n",
    "print('class weight for each label :' , class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b9104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - df: DataFrame containing image paths, labels, and bounding box coordinates.\n",
    "        - transforms: Any transformations to be applied to the images.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.df['path'] = '/Users/jaeeunlee/Documents/ds_study/ds_study/zerobase 프로젝트/deeplearning project/4. skin_digital/dataset/img/'+ df_forehead['image_name']\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Fetch the row corresponding to this index\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        # Extract image path and bounding box coordinates from the row\n",
    "        img_path = row['path']\n",
    "        bbox_x_min = int(row['bbox_x_min'])\n",
    "        bbox_y_min = int(row['bbox_y_min'])\n",
    "        bbox_x_max = int(row['bbox_x_max'])\n",
    "        bbox_y_max = int(row['bbox_y_max'])\n",
    "\n",
    "        # Read and crop the image using cv2\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        \n",
    "        # Crop the image using the bounding box coordinates\n",
    "        cropped_image = image[bbox_y_min:bbox_y_max, bbox_x_min:bbox_x_max]\n",
    "        \n",
    "        # Apply any transformations if they are provided\n",
    "        if self.transforms is not None:\n",
    "            cropped_image = self.transforms(cropped_image)  # 이미지 자체를 전달\n",
    "        \n",
    "        # If the label is available, return the cropped image and the label\n",
    "        if 'forehead_wrinkle' in row:\n",
    "            label = row['adj_forehead_wrinkle_2']\n",
    "            return cropped_image, label\n",
    "        else:\n",
    "            return cropped_image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46acc0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "31\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Example transformations (you can modify as needed)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomDataset(df=df_forehead, transforms=data_transforms)  # --> transform 나중에 해도 되는거 아닌가..? 그래야 보여줄 수 있을 거 같은뎅.....\n",
    "\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Example of how to get an image and label\n",
    "cropped_image, label = dataset[0]\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(len(train_loader)) #4825 * 0.8 / 32 = 121\n",
    "print(len(val_loader)) #4825 * 0.2 / 32 = 31\n",
    "\n",
    "#첫 한세트 테스트 하기 \n",
    "for images, labels in train_loader :\n",
    "    break\n",
    "\n",
    "print(images.shape) #1계 텐서 변환 전(변환 ㅅ)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26aee733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testset(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            predicted = torch.max(outputs, 1)[1]\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_acc += (predicted == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "    avg_test_loss = test_loss / total\n",
    "    avg_test_acc = test_acc / total\n",
    "\n",
    "    print(f'Test loss: {avg_test_loss:.5f}, Test accuracy: {avg_test_acc:.5f}')\n",
    "    return avg_test_loss, avg_test_acc, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc221df6",
   "metadata": {},
   "source": [
    "#### 뭔가 이상하게 잘 안풀리는 중인데..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56dcf895",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MobileNetV2:\n\tUnexpected key(s) in state_dict: \"classifier.4.weight\", \"classifier.4.bias\", \"classifier.5.weight\", \"classifier.5.bias\", \"classifier.5.running_mean\", \"classifier.5.running_var\", \"classifier.5.num_batches_tracked\", \"classifier.8.weight\", \"classifier.8.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.1.num_batches_tracked\". \n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([4, 1280]).\n\tsize mismatch for classifier.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mmobilenet_v2(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mlast_channel, \u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# 클래스 수 4로 설정\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmobilnet_v2_trained_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2.46\u001b[39m, \u001b[38;5;241m2.46\u001b[39m, \u001b[38;5;241m12.7\u001b[39m, \u001b[38;5;241m9.28\u001b[39m]) \u001b[38;5;66;03m#이마 class_weight\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds_study/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MobileNetV2:\n\tUnexpected key(s) in state_dict: \"classifier.4.weight\", \"classifier.4.bias\", \"classifier.5.weight\", \"classifier.5.bias\", \"classifier.5.running_mean\", \"classifier.5.running_var\", \"classifier.5.num_batches_tracked\", \"classifier.8.weight\", \"classifier.8.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.1.num_batches_tracked\". \n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([4, 1280]).\n\tsize mismatch for classifier.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([4])."
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.mobilenet_v2(pretrained=False)\n",
    "model.classifier[1] = nn.Linear(model.last_channel, 4)  # 클래스 수 4로 설정\n",
    "model.load_state_dict(torch.load(\"mobilnet_v2_trained_model.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "weights = torch.tensor([2.46, 2.46, 12.7, 9.28]) #이마 class_weight\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, reduction='mean')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf1367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
